{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# AMP components (GradScaler, autocast) are removed\n",
    "from torchvision import transforms, models, datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import logging # Logging\n",
    "from tqdm import tqdm # Use standard tqdm for console\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Path to your FER2013 CSV file\n",
    "#CSV_PATH = \"../../fer2013.csv\"\n",
    "# Model/Checkpoint saving directory\n",
    "local_time = time.localtime()\n",
    "current_date = time.strftime(\"%Y-%m-%d\", local_time)\n",
    "MODEL_DIR = \"models_checkpointed\"\n",
    "# Log file path\n",
    "LOG_FILE = \"training_log.log\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# --- Basic Logging Setup ---\n",
    "# Remove existing handlers if any to avoid duplicate logging on re-runs in some environments\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE, mode='w'), # mode='w' to overwrite log file on each run\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logging.info(f\"Logging to console and {LOG_FILE}\")\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    logging.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "BASE_MODEL_NAME = \"mnetv3_s\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 7 # Increased\n",
    "EPOCHS_PHASE2 = 20 # Increased\n",
    "K_FOLDS = 2\n",
    "LEARNING_RATE_HEAD = 1e-3\n",
    "LEARNING_RATE_BACKBONE = 1e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "SCHEDULER_PATIENCE = 3\n",
    "SCHEDULER_FACTOR = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 7 # Increased\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "# --- Log Hyperparameters ---\n",
    "logging.info(\"--- Hyperparameters ---\")\n",
    "logging.info(f\"Base Model: {BASE_MODEL_NAME}\")\n",
    "logging.info(f\"Batch Size: {BATCH_SIZE}\")\n",
    "logging.info(f\"Epochs Phase 1 (Head Training): {EPOCHS_PHASE1}\")\n",
    "logging.info(f\"Epochs Phase 2 (Full Model Fine-tuning): {EPOCHS_PHASE2}\")\n",
    "logging.info(f\"K-Folds for Cross-Validation: {K_FOLDS}\")\n",
    "logging.info(f\"Learning Rate (Head): {LEARNING_RATE_HEAD}\")\n",
    "logging.info(f\"Learning Rate (Backbone): {LEARNING_RATE_BACKBONE}\")\n",
    "logging.info(f\"Weight Decay: {WEIGHT_DECAY}\")\n",
    "logging.info(f\"Scheduler Patience: {SCHEDULER_PATIENCE}\")\n",
    "logging.info(f\"Scheduler Factor: {SCHEDULER_FACTOR}\")\n",
    "logging.info(f\"Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "logging.info(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "logging.info(f\"Random Seed: {SEED}\")\n",
    "logging.info(f\"AMP (Mixed Precision): False\") # Explicitly state AMP is off\n",
    "logging.info(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Load Data Img ---\n",
    "dataset_dir = \"fer2013/\"\n",
    "TRAIN_DIR = os.path.join(dataset_dir, \"train\")\n",
    "TEST_DIR = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "logging.info(f\"Loading data from {dataset_dir}\")\n",
    "\n",
    "def load_data_from_dirs(directory_path, positive_class_str=\"happy\"):\n",
    "    data = []\n",
    "    supported_extensions = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\")\n",
    "\n",
    "    if not os.path.isdir(directory_path):\n",
    "        logging.error(f\"Directory not found: {directory_path}\")\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"filepath\", \"emotion\", \"label\"]\n",
    "        )  # Return empty DataFrame\n",
    "\n",
    "    for emotion_name in os.listdir(directory_path):\n",
    "        emotion_subdir_path = os.path.join(directory_path, emotion_name)\n",
    "        if os.path.isdir(emotion_subdir_path):\n",
    "            # Determine binary label based on the original logic (emotion == 3)\n",
    "            label = 1 if emotion_name == positive_class_str else 0\n",
    "            for filename in os.listdir(emotion_subdir_path):\n",
    "                if filename.lower().endswith(supported_extensions):\n",
    "                    filepath = os.path.join(emotion_subdir_path, filename)\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"filepath\": filepath,\n",
    "                            \"emotion\": emotion_name,  # Original emotion string\n",
    "                            \"label\": label,  # Binary label\n",
    "                        }\n",
    "                    )\n",
    "    if not data:\n",
    "        logging.warning(\n",
    "            f\"No image files found in {directory_path} or its subdirectories.\"\n",
    "        )\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load training/validation data\n",
    "df_train_val = load_data_from_dirs(TRAIN_DIR, positive_class_str=\"happy\")\n",
    "if df_train_val.empty and not os.path.isdir(TRAIN_DIR):\n",
    "    logging.error(\n",
    "        f\"Critical error: Training data directory not found or inaccessible: {TRAIN_DIR}. Exiting.\"\n",
    "    )\n",
    "    exit()\n",
    "elif df_train_val.empty:\n",
    "    logging.warning(\n",
    "        f\"No training/validation data loaded from {TRAIN_DIR}. Proceeding with an empty training/validation set.\"\n",
    "    )\n",
    "\n",
    "# Load test data\n",
    "df_test = load_data_from_dirs(TEST_DIR, positive_class_str=\"happy\")\n",
    "if df_test.empty and not os.path.isdir(TEST_DIR):\n",
    "    logging.error(\n",
    "        f\"Critical error: Test data directory not found or inaccessible: {TEST_DIR}. Exiting.\"\n",
    "    )\n",
    "    exit()\n",
    "elif df_test.empty:\n",
    "    logging.warning(\n",
    "        f\"No test data loaded from {TEST_DIR}. Proceeding with an empty test set.\"\n",
    "    )\n",
    "\n",
    "logging.info(f\"Loaded {len(df_train_val)} samples for Training/Validation\")\n",
    "logging.info(f\"Loaded {len(df_test)} samples for Final Testing\")\n",
    "\n",
    "# Class weights calculation for df_train_val\n",
    "# The 'label' column is already created by load_data_from_dirs\n",
    "if not df_train_val.empty and \"label\" in df_train_val.columns:\n",
    "    class_counts = df_train_val[\"label\"].value_counts().sort_index()\n",
    "    logging.info(\n",
    "        f\"Class counts (Train/Val): {class_counts.to_dict() if not class_counts.empty else 'No data for class counts'}\"\n",
    "    )\n",
    "\n",
    "    use_equal_weights = False\n",
    "    # We expect two classes (0 and 1) for binary classification\n",
    "    expected_classes = {0, 1}\n",
    "    present_classes = set(class_counts.index)\n",
    "\n",
    "    if not expected_classes.issubset(present_classes) or len(present_classes) < 2 :\n",
    "        logging.warning(\n",
    "            f\"Expected classes {expected_classes} but found {present_classes} in training/validation data. \"\n",
    "            \"Using equal class weights [1.0, 1.0].\"\n",
    "        )\n",
    "        use_equal_weights = True\n",
    "    elif class_counts.get(0, 0) == 0 or class_counts.get(1, 0) == 0:\n",
    "        logging.warning(\n",
    "            \"One of the expected binary classes (0 or 1) has zero samples in training/validation data. \"\n",
    "            \"Using equal class weights [1.0, 1.0].\"\n",
    "        )\n",
    "        use_equal_weights = True\n",
    "\n",
    "    if use_equal_weights:\n",
    "        class_weights_tensor = torch.tensor([1.0, 1.0], dtype=torch.float32)\n",
    "    else:\n",
    "        total_samples = len(df_train_val)\n",
    "        num_binary_classes = 2  # For classes 0 and 1\n",
    "\n",
    "        # Calculate weights: total_samples / (num_classes * count_for_class_i)\n",
    "        # Ensure order is [weight_for_class_0, weight_for_class_1]\n",
    "        weight_for_class_0 = total_samples / (\n",
    "            num_binary_classes * class_counts.get(0, 1)\n",
    "        )  # Avoid division by zero if somehow count is 0\n",
    "        weight_for_class_1 = total_samples / (\n",
    "            num_binary_classes * class_counts.get(1, 1)\n",
    "        )\n",
    "\n",
    "        # Safety check for counts, though 'use_equal_weights' should prevent 0 counts here\n",
    "        if class_counts.get(0,0) == 0 or class_counts.get(1,0) == 0:\n",
    "             logging.error(\"Logic error: Attempting to calculate weights with zero count for a class. Defaulting to equal weights.\")\n",
    "             class_weights_tensor = torch.tensor([1.0, 1.0], dtype=torch.float32)\n",
    "        else:\n",
    "            class_weights_tensor = torch.tensor(\n",
    "                [weight_for_class_0, weight_for_class_1], dtype=torch.float32\n",
    "            )\n",
    "    logging.info(f\"Calculated class weights: {class_weights_tensor.numpy()}\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"Training/Validation DataFrame is empty or 'label' column is missing. \"\n",
    "        \"Skipping class weight calculation and using default equal weights [1.0, 1.0].\"\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        [1.0, 1.0], dtype=torch.float32\n",
    "    ) # Default equal weights\n",
    "    logging.info(f\"Using default class weights: {class_weights_tensor.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Dataset Class ---\n",
    "class FERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FERDataset that loads images from file paths.\n",
    "    Expects DataFrame columns: 'filepath', 'label'\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        if \"filepath\" not in self.df.columns:\n",
    "            raise ValueError(f\"'filepath' column not found in DataFrame columns: {self.df.columns.tolist()}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
    "        label = row[\"label\"]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout_rate_param=DROPOUT_RATE):\n",
    "    logging.info(f\"Initializing {BASE_MODEL_NAME} model.\")\n",
    "    model = models.mobilenet_v3_small(pretrained=True)\n",
    "    \n",
    "    # MobileNetV3's classifier is usually: [Linear, Hardswish, Dropout, Linear]\n",
    "    if hasattr(model, 'classifier'):\n",
    "        # Adjust dropout rate if present\n",
    "        for layer in model.classifier:\n",
    "            if isinstance(layer, nn.Dropout):\n",
    "                layer.p = dropout_rate_param\n",
    "                logging.info(f\"Adjusted Dropout rate in classifier to {dropout_rate_param}\")\n",
    "\n",
    "        # Replace the final linear layer for binary classification\n",
    "        if isinstance(model.classifier[-1], nn.Linear):\n",
    "            in_features = model.classifier[-1].in_features\n",
    "            model.classifier[-1] = nn.Linear(in_features, 2)\n",
    "            logging.info(\"Replaced final classifier layer for 2 output classes.\")\n",
    "        else:\n",
    "            logging.error(\"Final layer of classifier is not Linear as expected.\")\n",
    "            raise AttributeError(\"Unexpected classifier structure in MobileNetV3.\")\n",
    "    else:\n",
    "        logging.error(f\"Model {BASE_MODEL_NAME} does not have a 'classifier' attribute.\")\n",
    "        raise AttributeError(f\"Model {BASE_MODEL_NAME} structure not as expected.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training & Evaluation Functions ---\n",
    "def train_one_epoch(\n",
    "    model, loader, criterion, optimizer, device, phase=\"Head\" # scaler removed\n",
    "):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch ({phase})\", leave=False)\n",
    "    for i, (imgs, labels) in enumerate(pbar):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        try:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during forward/loss calculation in training batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during backward/step in training batch {i}: {e}\")\n",
    "             optimizer.zero_grad(set_to_none=True)\n",
    "             continue\n",
    "\n",
    "        if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        else:\n",
    "            logging.warning(f\"NaN or Inf loss detected in training batch {i}. Skipping accumulation.\")\n",
    "        pbar.set_postfix(loss=loss.item() if not torch.isnan(loss) else float('nan'))\n",
    "\n",
    "    if not loader.dataset or len(loader.dataset) == 0: return 0.0\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device): # scaler removed\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    all_probs = []\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, labels) in enumerate(pbar):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            try:\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    running_loss += loss.item() * imgs.size(0)\n",
    "                else:\n",
    "                    logging.warning(f\"NaN or Inf loss detected in evaluation batch {i}. Skipping accumulation.\")\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probabilities.cpu().numpy())\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during evaluation batch {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not loader.dataset or not all_targets:\n",
    "        logging.warning(\"Evaluation dataset empty or all batches failed.\")\n",
    "        return {\n",
    "            \"loss\": float('inf'), \"accuracy\": 0.0, \"f1\": 0.0, \"precision\": 0.0,\n",
    "            \"recall\": 0.0, \"auc\": 0.0, \"cm\": np.zeros((2, 2)), \"report\": {},\n",
    "            \"preds\": np.array([]), \"targets\": np.array([])\n",
    "        }\n",
    "\n",
    "    val_loss = running_loss / len(all_targets) if len(all_targets) > 0 else float('inf')\n",
    "    all_targets_np = np.array(all_targets)\n",
    "    all_preds_np = np.array(all_preds)\n",
    "    all_probs_np = np.array(all_probs)\n",
    "\n",
    "    val_acc = accuracy_score(all_targets_np, all_preds_np) if len(all_targets_np) > 0 else 0.0\n",
    "    val_f1 = f1_score(all_targets_np, all_preds_np, average=\"binary\", zero_division=0) if len(all_targets_np) > 0 else 0.0\n",
    "    val_prec = precision_score(all_targets_np, all_preds_np, average=\"binary\", zero_division=0) if len(all_targets_np) > 0 else 0.0\n",
    "    val_rec = recall_score(all_targets_np, all_preds_np, average=\"binary\", zero_division=0) if len(all_targets_np) > 0 else 0.0\n",
    "    val_auc = 0.0\n",
    "    try:\n",
    "        if len(np.unique(all_targets_np)) > 1: # Check for at least two classes for AUC\n",
    "             val_auc = roc_auc_score(all_targets_np, all_probs_np)\n",
    "        elif len(all_targets_np) > 0: # Only one class present\n",
    "             logging.warning(\"AUC calculation skipped: only one class present in evaluation targets.\")\n",
    "    except ValueError as e: # Catch specific sklearn error if all predictions are one class\n",
    "        logging.warning(f\"AUC calculation failed (ValueError): {e}\")\n",
    "    except Exception as e: # Catch any other unexpected error\n",
    "        logging.error(f\"AUC calculation failed (General Error): {e}\")\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(all_targets_np, all_preds_np) if len(all_targets_np) > 0 else np.zeros((2,2))\n",
    "    report_dict = {}\n",
    "    if len(all_targets_np) > 0:\n",
    "        try:\n",
    "            report_dict = classification_report(\n",
    "                    all_targets_np, all_preds_np, target_names=[\"Not Happy\", \"Happy\"], output_dict=True, zero_division=0\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating classification report: {e}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": val_loss, \"accuracy\": val_acc, \"f1\": val_f1, \"precision\": val_prec,\n",
    "        \"recall\": val_rec, \"auc\": val_auc, \"cm\": cm, \"report\": report_dict,\n",
    "        \"preds\": all_preds_np, \"targets\": all_targets_np\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def evaluate_loss_only(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                running_loss += loss.item() * imgs.size(0)\n",
    "                total_samples += imgs.size(0)\n",
    "    if total_samples == 0:\n",
    "        return float('inf')\n",
    "    return running_loss / total_samples\n",
    "\n",
    "\n",
    "# --- Main K-Fold Cross-Validation ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "num_workers = 4 if os.name != \"nt\" and torch.cuda.is_available() else 0\n",
    "logging.info(f\"Using {num_workers} workers for DataLoaders\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "X_data = df_train_val.index.values\n",
    "y_data = df_train_val[\"label\"].values\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "    logging.info(f\"--- Starting Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    fold_start_time = time.time()\n",
    "\n",
    "    train_df = df_train_val.iloc[train_idx]\n",
    "    val_df = df_train_val.iloc[val_idx]\n",
    "\n",
    "    train_ds = FERDataset(train_df, transform=train_transform)\n",
    "    val_ds = FERDataset(val_df, transform=val_test_transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False, drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=False\n",
    "    )\n",
    "\n",
    "    model = get_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n",
    "\n",
    "    # --- Phase 1: Train the Head ---\n",
    "    logging.info(\"--- Phase 1: Training Head ---\")\n",
    "    if hasattr(model, 'features'):\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    if hasattr(model, 'classifier'):\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=LEARNING_RATE_HEAD,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS_PHASE1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, phase=\"Head\"\n",
    "        )\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        logging.info(\n",
    "            f\"Fold {fold+1} Phase 1 - Epoch {epoch+1}/{EPOCHS_PHASE1}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # --- Phase 2: Fine-tune the whole model ---\n",
    "    logging.info(\"--- Phase 2: Fine-tuning Full Model ---\")\n",
    "    if hasattr(model, 'features'):\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\n",
    "            \"params\": model.features.parameters() if hasattr(model, 'features') else [],\n",
    "            \"lr\": LEARNING_RATE_BACKBONE,\n",
    "        },\n",
    "        {\n",
    "            \"params\": model.classifier.parameters() if hasattr(model, 'classifier') else model.parameters(),\n",
    "            \"lr\": LEARNING_RATE_HEAD,\n",
    "        },\n",
    "    ],\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_epoch = -1\n",
    "    checkpoint_path = os.path.join(MODEL_DIR, f\"best_model_fold_{fold+1}.pth\")\n",
    "\n",
    "    for epoch in range(EPOCHS_PHASE2):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_total = EPOCHS_PHASE1 + epoch + 1\n",
    "\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, phase=\"Full\"\n",
    "        )\n",
    "        val_loss = evaluate_loss_only(model, val_loader, criterion, device)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        logging.info(\n",
    "            f\"Fold {fold+1} Phase 2 - Epoch {epoch+1}/{EPOCHS_PHASE2} (Total: {current_epoch_total}), \"\n",
    "            f\"LR: {current_lrs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "            f\"Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_epoch = current_epoch_total\n",
    "            try:\n",
    "                torch.save({\n",
    "                    'epoch': current_epoch_total,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                }, checkpoint_path)\n",
    "                logging.info(\n",
    "                    f\"  -> New best Val Loss: {best_val_loss:.4f} at epoch {best_epoch}. Checkpoint saved to {checkpoint_path}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving checkpoint: {e}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            logging.info(f\"  -> Val Loss did not improve. Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            logging.info(f\"  -> Early stopping triggered at epoch {current_epoch_total}.\")\n",
    "            break\n",
    "\n",
    "    # Load best model and do full evaluation (all metrics) at the end of the fold\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        logging.info(f\"Loading best model from {checkpoint_path} (Epoch {best_epoch}, Val Loss: {best_val_loss:.4f})\")\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading checkpoint: {e}. Using last model state.\")\n",
    "    else:\n",
    "        logging.warning(\"No best model checkpoint found for this fold. Using last model state.\")\n",
    "\n",
    "    logging.info(f\"--- Evaluating Best Model for Fold {fold+1} ---\")\n",
    "    final_fold_metrics = evaluate(model, val_loader, criterion, device)  # Full metrics only here\n",
    "\n",
    "    logging.info(f\"Fold {fold+1} Final Validation Results (Best Model):\")\n",
    "    logging.info(f\"  Accuracy:  {final_fold_metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"  F1 Score:  {final_fold_metrics['f1']:.4f}\")\n",
    "    logging.info(f\"  Precision: {final_fold_metrics['precision']:.4f}\")\n",
    "    logging.info(f\"  Recall:    {final_fold_metrics['recall']:.4f}\")\n",
    "    logging.info(f\"  AUC:       {final_fold_metrics['auc']:.4f}\")\n",
    "    logging.info(f\"  Loss:      {final_fold_metrics['loss']:.4f}\")\n",
    "    logging.info(\"  Confusion Matrix:\")\n",
    "    logging.info(f\"\\n{final_fold_metrics['cm']}\")\n",
    "\n",
    "    fold_results.append(final_fold_metrics)\n",
    "    fold_time = time.time() - fold_start_time\n",
    "    logging.info(f\"--- Fold {fold+1} completed in {fold_time:.2f}s ---\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-Validation Summary ---\n",
    "logging.info(\"--- Cross-Validation Summary ---\")\n",
    "if fold_results:\n",
    "    accs = [r[\"accuracy\"] for r in fold_results]\n",
    "    f1s = [r[\"f1\"] for r in fold_results]\n",
    "    precs = [r[\"precision\"] for r in fold_results]\n",
    "    recs = [r[\"recall\"] for r in fold_results]\n",
    "    aucs = [r[\"auc\"] for r in fold_results]\n",
    "\n",
    "    logging.info(f\"Mean Accuracy:  {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    logging.info(f\"Mean F1 Score:  {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n",
    "    logging.info(f\"Mean Precision: {np.mean(precs):.4f} ± {np.std(precs):.4f}\")\n",
    "    logging.info(f\"Mean Recall:    {np.mean(recs):.4f} ± {np.std(recs):.4f}\")\n",
    "    logging.info(f\"Mean AUC:       {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "else:\n",
    "    logging.warning(\"No fold results to summarize.\")\n",
    "\n",
    "# --- Training Final Model on Full Train/Val Data ---\n",
    "logging.info(\"--- Training Final Model on Full Train/Val Dataset ---\")\n",
    "final_model = get_model().to(device)\n",
    "final_ds = FERDataset(df_train_val, transform=train_transform)\n",
    "final_loader = DataLoader(\n",
    "    final_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False, drop_last=True\n",
    ")\n",
    "criterion_final = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n",
    "# scaler_final is removed\n",
    "\n",
    "# --- Final Model - Phase 1: Train the Head ---\n",
    "logging.info(\"--- Final Model - Phase 1: Training Head ---\")\n",
    "if hasattr(final_model, 'features'):\n",
    "    for param in final_model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "if hasattr(final_model, 'classifier'):\n",
    "    for param in final_model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer_head_final = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, final_model.parameters()),\n",
    "    lr=LEARNING_RATE_HEAD,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "for epoch in range(EPOCHS_PHASE1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_one_epoch(\n",
    "        final_model, final_loader, criterion_final, optimizer_head_final, device, phase=\"Head\" # scaler removed\n",
    "    )\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    logging.info(\n",
    "        f\"Final Phase 1 - Epoch {epoch+1}/{EPOCHS_PHASE1}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s\"\n",
    "    )\n",
    "\n",
    "# --- Final Model - Phase 2: Fine-tune the whole model ---\n",
    "logging.info(\"--- Final Model - Phase 2: Fine-tuning Full Model ---\")\n",
    "if hasattr(final_model, 'features'):\n",
    "    for param in final_model.features.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\n",
    "            \"params\": model.features.parameters() if hasattr(model, 'features') else [],\n",
    "            \"lr\": LEARNING_RATE_BACKBONE,\n",
    "        },\n",
    "        {\n",
    "            \"params\": model.classifier.parameters() if hasattr(model, 'classifier') else model.parameters(),\n",
    "            \"lr\": LEARNING_RATE_HEAD,\n",
    "        },\n",
    "    ],\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Optionally, you can use a validation split from df_train_val for early stopping,\n",
    "# or just train for all epochs if you want to use all data.\n",
    "\n",
    "for epoch in range(EPOCHS_PHASE2):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_one_epoch(\n",
    "        final_model, final_loader, criterion_final, optimizer_full_final, device, phase=\"Full\"\n",
    "    )\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    logging.info(\n",
    "        f\"Final Phase 2 - Epoch {epoch+1}/{EPOCHS_PHASE2}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s\"\n",
    "    )\n",
    "\n",
    "final_model_save_path = os.path.join(MODEL_DIR, f\"{BASE_MODEL_NAME.lower()}_fer2013_happy_final.pth\")\n",
    "try:\n",
    "    torch.save(final_model.state_dict(), final_model_save_path)\n",
    "    logging.info(f\"Final model state_dict saved to {final_model_save_path}\")\n",
    "    del final_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving final model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Evaluation on Test Set ---\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "final_model_save_path=os.path.join(MODEL_DIR, f\"{BASE_MODEL_NAME.lower()}_fer2013_happy_final.pth\")\n",
    "logging.info(\"--- Evaluating Final Model on PrivateTest Set ---\")\n",
    "if len(df_test) > 0:\n",
    "    test_ds = FERDataset(df_test, transform=val_test_transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        eval_model = get_model().to(device) # Create a new model instance for evaluation\n",
    "        eval_model.load_state_dict(torch.load(final_model_save_path, map_location=device))\n",
    "        eval_model.eval() # Ensure model is in eval mode\n",
    "        logging.info(f\"Successfully loaded final model from {final_model_save_path}\")\n",
    "\n",
    "        # 1. Standard Metrics Evaluation\n",
    "        logging.info(\"--- Calculating Performance Metrics on Test Set ---\")\n",
    "        test_metrics = evaluate(eval_model, test_loader, criterion_final, device)\n",
    "\n",
    "        logging.info(\"Final Test Set Performance Metrics:\")\n",
    "        logging.info(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "        logging.info(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "        logging.info(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "        logging.info(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "        logging.info(f\"  AUC:       {test_metrics['auc']:.4f}\")\n",
    "        logging.info(f\"  Loss:      {test_metrics['loss']:.4f}\")\n",
    "        logging.info(\"  Confusion Matrix:\")\n",
    "        logging.info(f\"\\n{test_metrics['cm']}\")\n",
    "        logging.info(\"  Classification Report:\")\n",
    "        if test_metrics['targets'].size > 0:\n",
    "             logging.info(f\"\\n{classification_report(test_metrics['targets'], test_metrics['preds'], target_names=['Not Happy', 'Happy'], zero_division=0)}\")\n",
    "        else:\n",
    "             logging.warning(\"No predictions generated during final test evaluation for report.\")\n",
    "\n",
    "        # 2. Inference Speed Test\n",
    "        logging.info(\"--- Performing Inference Speed Test on Test Set ---\")\n",
    "        dummy_batch = None\n",
    "        try:\n",
    "            dummy_batch = next(iter(test_loader)) # Get one batch for warm-up\n",
    "        except StopIteration:\n",
    "            logging.warning(\"Test loader is empty, cannot perform warm-up for speed test.\")\n",
    "        \n",
    "        if dummy_batch:\n",
    "            dummy_imgs, _ = dummy_batch\n",
    "            dummy_imgs = dummy_imgs.to(device)\n",
    "\n",
    "            # Warm-up iterations\n",
    "            logging.info(\"Performing warm-up inferences...\")\n",
    "            with torch.no_grad():\n",
    "                for _ in range(5): # Number of warm-up iterations\n",
    "                    _ = eval_model(dummy_imgs)\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "            logging.info(\"Warm-up complete.\")\n",
    "\n",
    "        total_inference_time = 0\n",
    "        total_images_processed = 0\n",
    "        inference_times_per_batch = []\n",
    "\n",
    "        logging.info(\"Starting timed inferences...\")\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in tqdm(test_loader, desc=\"Inference Speed Test\"):\n",
    "                imgs = imgs.to(device)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                start_time = time.perf_counter() # More precise timer\n",
    "\n",
    "                _ = eval_model(imgs)\n",
    "\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                end_time = time.perf_counter()\n",
    "\n",
    "                batch_time = end_time - start_time\n",
    "                inference_times_per_batch.append(batch_time)\n",
    "                total_inference_time += batch_time\n",
    "                total_images_processed += imgs.size(0)\n",
    "        \n",
    "        if total_images_processed > 0:\n",
    "            avg_time_per_image = total_inference_time / total_images_processed\n",
    "            images_per_second = total_images_processed / total_inference_time\n",
    "            avg_time_per_batch = total_inference_time / len(test_loader)\n",
    "\n",
    "            logging.info(\"Inference Speed Test Results:\")\n",
    "            logging.info(f\"  Total images processed: {total_images_processed}\")\n",
    "            logging.info(f\"  Total inference time: {total_inference_time:.4f} seconds\")\n",
    "            logging.info(f\"  Average inference time per batch: {avg_time_per_batch:.6f} seconds (Batch Size: {BATCH_SIZE})\")\n",
    "            logging.info(f\"  Average inference time per image: {avg_time_per_image:.6f} seconds\")\n",
    "            logging.info(f\"  Images Per Second (IPS): {images_per_second:.2f}\")\n",
    "        else:\n",
    "            logging.warning(\"No images were processed during the inference speed test (Test set might be empty or all batches failed).\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Final model file not found at {final_model_save_path}. Cannot evaluate on test set.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during final evaluation or speed test on test set: {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logging.warning(\"PrivateTest set is empty or could not be loaded. Skipping final evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
