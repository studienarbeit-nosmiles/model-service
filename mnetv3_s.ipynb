{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# AMP components (GradScaler, autocast) are removed\n",
    "from torchvision import transforms, models, datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import logging # Logging\n",
    "from tqdm import tqdm # Use standard tqdm for console\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 17:21:11,509 [INFO] Logging to console and training_log.log\n",
      "2025-05-13 17:21:11,646 [INFO] Random seed set to 42\n",
      "2025-05-13 17:21:11,648 [INFO] --- Hyperparameters ---\n",
      "2025-05-13 17:21:11,649 [INFO] Base Model: mnetv3_s\n",
      "2025-05-13 17:21:11,650 [INFO] Batch Size: 32\n",
      "2025-05-13 17:21:11,651 [INFO] Epochs Phase 1 (Head Training): 7\n",
      "2025-05-13 17:21:11,651 [INFO] Epochs Phase 2 (Full Model Fine-tuning): 20\n",
      "2025-05-13 17:21:11,652 [INFO] K-Folds for Cross-Validation: 2\n",
      "2025-05-13 17:21:11,653 [INFO] Learning Rate (Head): 0.001\n",
      "2025-05-13 17:21:11,654 [INFO] Learning Rate (Backbone): 1e-05\n",
      "2025-05-13 17:21:11,654 [INFO] Weight Decay: 0.01\n",
      "2025-05-13 17:21:11,655 [INFO] Scheduler Patience: 3\n",
      "2025-05-13 17:21:11,656 [INFO] Scheduler Factor: 0.1\n",
      "2025-05-13 17:21:11,657 [INFO] Early Stopping Patience: 7\n",
      "2025-05-13 17:21:11,658 [INFO] Dropout Rate: 0.5\n",
      "2025-05-13 17:21:11,658 [INFO] Random Seed: 42\n",
      "2025-05-13 17:21:11,659 [INFO] AMP (Mixed Precision): False\n",
      "2025-05-13 17:21:11,660 [INFO] -----------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Path to your FER2013 CSV file\n",
    "#CSV_PATH = \"../../fer2013.csv\"\n",
    "# Model/Checkpoint saving directory\n",
    "local_time = time.localtime()\n",
    "current_date = time.strftime(\"%Y-%m-%d\", local_time)\n",
    "MODEL_DIR = \"models_checkpointed\"\n",
    "# Log file path\n",
    "LOG_FILE = \"training_log.log\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# --- Basic Logging Setup ---\n",
    "# Remove existing handlers if any to avoid duplicate logging on re-runs in some environments\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE, mode='w'), # mode='w' to overwrite log file on each run\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logging.info(f\"Logging to console and {LOG_FILE}\")\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    logging.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "BASE_MODEL_NAME = \"mnetv3_s\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 7 # Increased\n",
    "EPOCHS_PHASE2 = 20 # Increased\n",
    "K_FOLDS = 2\n",
    "LEARNING_RATE_HEAD = 1e-3\n",
    "LEARNING_RATE_BACKBONE = 1e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "SCHEDULER_PATIENCE = 3\n",
    "SCHEDULER_FACTOR = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 7 # Increased\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "# --- Log Hyperparameters ---\n",
    "logging.info(\"--- Hyperparameters ---\")\n",
    "logging.info(f\"Base Model: {BASE_MODEL_NAME}\")\n",
    "logging.info(f\"Batch Size: {BATCH_SIZE}\")\n",
    "logging.info(f\"Epochs Phase 1 (Head Training): {EPOCHS_PHASE1}\")\n",
    "logging.info(f\"Epochs Phase 2 (Full Model Fine-tuning): {EPOCHS_PHASE2}\")\n",
    "logging.info(f\"K-Folds for Cross-Validation: {K_FOLDS}\")\n",
    "logging.info(f\"Learning Rate (Head): {LEARNING_RATE_HEAD}\")\n",
    "logging.info(f\"Learning Rate (Backbone): {LEARNING_RATE_BACKBONE}\")\n",
    "logging.info(f\"Weight Decay: {WEIGHT_DECAY}\")\n",
    "logging.info(f\"Scheduler Patience: {SCHEDULER_PATIENCE}\")\n",
    "logging.info(f\"Scheduler Factor: {SCHEDULER_FACTOR}\")\n",
    "logging.info(f\"Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "logging.info(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "logging.info(f\"Random Seed: {SEED}\")\n",
    "logging.info(f\"AMP (Mixed Precision): False\") # Explicitly state AMP is off\n",
    "logging.info(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 17:21:32,788 [INFO] Loading data from fer2013/\n",
      "2025-05-13 17:21:33,012 [INFO] Loaded 28709 samples for Training/Validation\n",
      "2025-05-13 17:21:33,013 [INFO] Loaded 7178 samples for Final Testing\n",
      "2025-05-13 17:21:33,024 [INFO] Class counts (Train/Val): {0: 21494, 1: 7215}\n",
      "2025-05-13 17:21:33,032 [INFO] Calculated class weights: [0.66783756 1.9895357 ]\n"
     ]
    }
   ],
   "source": [
    "## --- Load Data Img ---\n",
    "dataset_dir = \"fer2013/\"\n",
    "TRAIN_DIR = os.path.join(dataset_dir, \"train\")\n",
    "TEST_DIR = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "logging.info(f\"Loading data from {dataset_dir}\")\n",
    "\n",
    "def load_data_from_dirs(directory_path, positive_class_str=\"happy\"):\n",
    "    data = []\n",
    "    supported_extensions = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\")\n",
    "\n",
    "    if not os.path.isdir(directory_path):\n",
    "        logging.error(f\"Directory not found: {directory_path}\")\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"filepath\", \"emotion\", \"label\"]\n",
    "        )  # Return empty DataFrame\n",
    "\n",
    "    for emotion_name in os.listdir(directory_path):\n",
    "        emotion_subdir_path = os.path.join(directory_path, emotion_name)\n",
    "        if os.path.isdir(emotion_subdir_path):\n",
    "            # Determine binary label based on the original logic (emotion == 3)\n",
    "            label = 1 if emotion_name == positive_class_str else 0\n",
    "            for filename in os.listdir(emotion_subdir_path):\n",
    "                if filename.lower().endswith(supported_extensions):\n",
    "                    filepath = os.path.join(emotion_subdir_path, filename)\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"filepath\": filepath,\n",
    "                            \"emotion\": emotion_name,  # Original emotion string\n",
    "                            \"label\": label,  # Binary label\n",
    "                        }\n",
    "                    )\n",
    "    if not data:\n",
    "        logging.warning(\n",
    "            f\"No image files found in {directory_path} or its subdirectories.\"\n",
    "        )\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load training/validation data\n",
    "df_train_val = load_data_from_dirs(TRAIN_DIR, positive_class_str=\"happy\")\n",
    "if df_train_val.empty and not os.path.isdir(TRAIN_DIR):\n",
    "    logging.error(\n",
    "        f\"Critical error: Training data directory not found or inaccessible: {TRAIN_DIR}. Exiting.\"\n",
    "    )\n",
    "    exit()\n",
    "elif df_train_val.empty:\n",
    "    logging.warning(\n",
    "        f\"No training/validation data loaded from {TRAIN_DIR}. Proceeding with an empty training/validation set.\"\n",
    "    )\n",
    "\n",
    "# Load test data\n",
    "df_test = load_data_from_dirs(TEST_DIR, positive_class_str=\"happy\")\n",
    "if df_test.empty and not os.path.isdir(TEST_DIR):\n",
    "    logging.error(\n",
    "        f\"Critical error: Test data directory not found or inaccessible: {TEST_DIR}. Exiting.\"\n",
    "    )\n",
    "    exit()\n",
    "elif df_test.empty:\n",
    "    logging.warning(\n",
    "        f\"No test data loaded from {TEST_DIR}. Proceeding with an empty test set.\"\n",
    "    )\n",
    "\n",
    "logging.info(f\"Loaded {len(df_train_val)} samples for Training/Validation\")\n",
    "logging.info(f\"Loaded {len(df_test)} samples for Final Testing\")\n",
    "\n",
    "# Class weights calculation for df_train_val\n",
    "# The 'label' column is already created by load_data_from_dirs\n",
    "if not df_train_val.empty and \"label\" in df_train_val.columns:\n",
    "    class_counts = df_train_val[\"label\"].value_counts().sort_index()\n",
    "    logging.info(\n",
    "        f\"Class counts (Train/Val): {class_counts.to_dict() if not class_counts.empty else 'No data for class counts'}\"\n",
    "    )\n",
    "\n",
    "    use_equal_weights = False\n",
    "    # We expect two classes (0 and 1) for binary classification\n",
    "    expected_classes = {0, 1}\n",
    "    present_classes = set(class_counts.index)\n",
    "\n",
    "    if not expected_classes.issubset(present_classes) or len(present_classes) < 2 :\n",
    "        logging.warning(\n",
    "            f\"Expected classes {expected_classes} but found {present_classes} in training/validation data. \"\n",
    "            \"Using equal class weights [1.0, 1.0].\"\n",
    "        )\n",
    "        use_equal_weights = True\n",
    "    elif class_counts.get(0, 0) == 0 or class_counts.get(1, 0) == 0:\n",
    "        logging.warning(\n",
    "            \"One of the expected binary classes (0 or 1) has zero samples in training/validation data. \"\n",
    "            \"Using equal class weights [1.0, 1.0].\"\n",
    "        )\n",
    "        use_equal_weights = True\n",
    "\n",
    "    if use_equal_weights:\n",
    "        class_weights_tensor = torch.tensor([1.0, 1.0], dtype=torch.float32)\n",
    "    else:\n",
    "        total_samples = len(df_train_val)\n",
    "        num_binary_classes = 2  # For classes 0 and 1\n",
    "\n",
    "        # Calculate weights: total_samples / (num_classes * count_for_class_i)\n",
    "        # Ensure order is [weight_for_class_0, weight_for_class_1]\n",
    "        weight_for_class_0 = total_samples / (\n",
    "            num_binary_classes * class_counts.get(0, 1)\n",
    "        )  # Avoid division by zero if somehow count is 0\n",
    "        weight_for_class_1 = total_samples / (\n",
    "            num_binary_classes * class_counts.get(1, 1)\n",
    "        )\n",
    "\n",
    "        # Safety check for counts, though 'use_equal_weights' should prevent 0 counts here\n",
    "        if class_counts.get(0,0) == 0 or class_counts.get(1,0) == 0:\n",
    "             logging.error(\"Logic error: Attempting to calculate weights with zero count for a class. Defaulting to equal weights.\")\n",
    "             class_weights_tensor = torch.tensor([1.0, 1.0], dtype=torch.float32)\n",
    "        else:\n",
    "            class_weights_tensor = torch.tensor(\n",
    "                [weight_for_class_0, weight_for_class_1], dtype=torch.float32\n",
    "            )\n",
    "    logging.info(f\"Calculated class weights: {class_weights_tensor.numpy()}\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"Training/Validation DataFrame is empty or 'label' column is missing. \"\n",
    "        \"Skipping class weight calculation and using default equal weights [1.0, 1.0].\"\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        [1.0, 1.0], dtype=torch.float32\n",
    "    ) # Default equal weights\n",
    "    logging.info(f\"Using default class weights: {class_weights_tensor.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Dataset Class ---\n",
    "class FERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FERDataset that loads images from file paths.\n",
    "    Expects DataFrame columns: 'filepath', 'label'\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        if \"filepath\" not in self.df.columns:\n",
    "            raise ValueError(f\"'filepath' column not found in DataFrame columns: {self.df.columns.tolist()}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
    "        label = row[\"label\"]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout_rate_param=DROPOUT_RATE):\n",
    "    logging.info(f\"Initializing {BASE_MODEL_NAME} model.\")\n",
    "    model = models.mobilenet_v3_small(pretrained=True)\n",
    "    \n",
    "    # MobileNetV3's classifier is usually: [Linear, Hardswish, Dropout, Linear]\n",
    "    if hasattr(model, 'classifier'):\n",
    "        # Adjust dropout rate if present\n",
    "        for layer in model.classifier:\n",
    "            if isinstance(layer, nn.Dropout):\n",
    "                layer.p = dropout_rate_param\n",
    "                logging.info(f\"Adjusted Dropout rate in classifier to {dropout_rate_param}\")\n",
    "\n",
    "        # Replace the final linear layer for binary classification\n",
    "        if isinstance(model.classifier[-1], nn.Linear):\n",
    "            in_features = model.classifier[-1].in_features\n",
    "            model.classifier[-1] = nn.Linear(in_features, 2)\n",
    "            logging.info(\"Replaced final classifier layer for 2 output classes.\")\n",
    "        else:\n",
    "            logging.error(\"Final layer of classifier is not Linear as expected.\")\n",
    "            raise AttributeError(\"Unexpected classifier structure in MobileNetV3.\")\n",
    "    else:\n",
    "        logging.error(f\"Model {BASE_MODEL_NAME} does not have a 'classifier' attribute.\")\n",
    "        raise AttributeError(f\"Model {BASE_MODEL_NAME} structure not as expected.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 17:21:42,246 [INFO] Using device: cuda\n",
      "2025-05-13 17:21:42,248 [INFO] Using 0 workers for DataLoaders\n",
      "2025-05-13 17:21:42,254 [INFO] --- Starting Fold 1/2 ---\n",
      "2025-05-13 17:21:42,258 [INFO] Initializing mnetv3_s model.\n",
      "c:\\Users\\Jerem\\Documents\\DHBW\\NoSmiles\\model-service\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jerem\\Documents\\DHBW\\NoSmiles\\model-service\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2025-05-13 17:21:42,379 [INFO] Adjusted Dropout rate in classifier to 0.5\n",
      "2025-05-13 17:21:42,380 [INFO] Replaced final classifier layer for 2 output classes.\n",
      "2025-05-13 17:21:42,536 [INFO] --- Phase 1: Training Head ---\n",
      "2025-05-13 17:22:37,621 [INFO] Fold 1 Phase 1 - Epoch 1/7, Train Loss: 0.5949, Time: 55.08s\n",
      "2025-05-13 17:23:15,069 [INFO] Fold 1 Phase 1 - Epoch 2/7, Train Loss: 0.5503, Time: 37.45s\n",
      "2025-05-13 17:23:52,710 [INFO] Fold 1 Phase 1 - Epoch 3/7, Train Loss: 0.5249, Time: 37.64s\n",
      "2025-05-13 17:24:30,097 [INFO] Fold 1 Phase 1 - Epoch 4/7, Train Loss: 0.5205, Time: 37.39s\n",
      "2025-05-13 17:25:07,915 [INFO] Fold 1 Phase 1 - Epoch 5/7, Train Loss: 0.5089, Time: 37.82s\n",
      "2025-05-13 17:25:45,725 [INFO] Fold 1 Phase 1 - Epoch 6/7, Train Loss: 0.5018, Time: 37.81s\n",
      "2025-05-13 17:26:23,444 [INFO] Fold 1 Phase 1 - Epoch 7/7, Train Loss: 0.4938, Time: 37.72s\n",
      "2025-05-13 17:26:23,444 [INFO] --- Phase 2: Fine-tuning Full Model ---\n",
      "2025-05-13 17:28:01,431 [INFO] Fold 1 Phase 2 - Epoch 1/20 (Total: 8), LR: [1e-05, 0.001], Train Loss: 0.4657, Val Loss: 0.3472, Time: 97.98s\n",
      "2025-05-13 17:28:01,555 [INFO]   -> New best Val Loss: 0.3472 at epoch 8. Checkpoint saved to models_checkpointed\\best_model_fold_1.pth\n",
      "2025-05-13 17:29:24,250 [INFO] Fold 1 Phase 2 - Epoch 2/20 (Total: 9), LR: [1e-05, 0.001], Train Loss: 0.4139, Val Loss: 0.3737, Time: 82.69s\n",
      "2025-05-13 17:29:24,251 [INFO]   -> Val Loss did not improve. Patience: 1/7\n",
      "2025-05-13 17:30:46,006 [INFO] Fold 1 Phase 2 - Epoch 3/20 (Total: 10), LR: [1e-05, 0.001], Train Loss: 0.3873, Val Loss: 0.4570, Time: 81.75s\n",
      "2025-05-13 17:30:46,008 [INFO]   -> Val Loss did not improve. Patience: 2/7\n",
      "2025-05-13 17:32:07,744 [INFO] Fold 1 Phase 2 - Epoch 4/20 (Total: 11), LR: [1e-05, 0.001], Train Loss: 0.3619, Val Loss: 0.3627, Time: 81.74s\n",
      "2025-05-13 17:32:07,745 [INFO]   -> Val Loss did not improve. Patience: 3/7\n",
      "2025-05-13 17:33:28,147 [INFO] Fold 1 Phase 2 - Epoch 5/20 (Total: 12), LR: [1e-05, 0.001], Train Loss: 0.3470, Val Loss: 0.3909, Time: 80.40s\n",
      "2025-05-13 17:33:28,149 [INFO]   -> Val Loss did not improve. Patience: 4/7\n",
      "2025-05-13 17:34:48,990 [INFO] Fold 1 Phase 2 - Epoch 6/20 (Total: 13), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.3124, Val Loss: 0.3372, Time: 80.84s\n",
      "2025-05-13 17:34:49,096 [INFO]   -> New best Val Loss: 0.3372 at epoch 13. Checkpoint saved to models_checkpointed\\best_model_fold_1.pth\n",
      "2025-05-13 17:36:14,174 [INFO] Fold 1 Phase 2 - Epoch 7/20 (Total: 14), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.3014, Val Loss: 0.3147, Time: 85.08s\n",
      "2025-05-13 17:36:14,282 [INFO]   -> New best Val Loss: 0.3147 at epoch 14. Checkpoint saved to models_checkpointed\\best_model_fold_1.pth\n",
      "2025-05-13 17:37:37,739 [INFO] Fold 1 Phase 2 - Epoch 8/20 (Total: 15), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.3063, Val Loss: 0.3181, Time: 83.45s\n",
      "2025-05-13 17:37:37,740 [INFO]   -> Val Loss did not improve. Patience: 1/7\n",
      "2025-05-13 17:39:00,045 [INFO] Fold 1 Phase 2 - Epoch 9/20 (Total: 16), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2983, Val Loss: 0.3306, Time: 82.30s\n",
      "2025-05-13 17:39:00,047 [INFO]   -> Val Loss did not improve. Patience: 2/7\n",
      "2025-05-13 17:40:22,982 [INFO] Fold 1 Phase 2 - Epoch 10/20 (Total: 17), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2972, Val Loss: 0.2972, Time: 82.93s\n",
      "2025-05-13 17:40:23,090 [INFO]   -> New best Val Loss: 0.2972 at epoch 17. Checkpoint saved to models_checkpointed\\best_model_fold_1.pth\n",
      "2025-05-13 17:41:46,342 [INFO] Fold 1 Phase 2 - Epoch 11/20 (Total: 18), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2980, Val Loss: 0.3270, Time: 83.25s\n",
      "2025-05-13 17:41:46,344 [INFO]   -> Val Loss did not improve. Patience: 1/7\n",
      "2025-05-13 17:43:10,688 [INFO] Fold 1 Phase 2 - Epoch 12/20 (Total: 19), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2950, Val Loss: 0.3179, Time: 84.34s\n",
      "2025-05-13 17:43:10,690 [INFO]   -> Val Loss did not improve. Patience: 2/7\n",
      "2025-05-13 17:44:34,205 [INFO] Fold 1 Phase 2 - Epoch 13/20 (Total: 20), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2858, Val Loss: 0.3242, Time: 83.51s\n",
      "2025-05-13 17:44:34,207 [INFO]   -> Val Loss did not improve. Patience: 3/7\n",
      "2025-05-13 17:45:57,150 [INFO] Fold 1 Phase 2 - Epoch 14/20 (Total: 21), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2910, Val Loss: 0.3046, Time: 82.94s\n",
      "2025-05-13 17:45:57,151 [INFO]   -> Val Loss did not improve. Patience: 4/7\n",
      "2025-05-13 17:47:19,042 [INFO] Fold 1 Phase 2 - Epoch 15/20 (Total: 22), LR: [1.0000000000000002e-07, 1e-05], Train Loss: 0.2843, Val Loss: 0.3117, Time: 81.89s\n",
      "2025-05-13 17:47:19,043 [INFO]   -> Val Loss did not improve. Patience: 5/7\n",
      "2025-05-13 17:48:40,262 [INFO] Fold 1 Phase 2 - Epoch 16/20 (Total: 23), LR: [1.0000000000000002e-07, 1e-05], Train Loss: 0.2834, Val Loss: 0.3134, Time: 81.22s\n",
      "2025-05-13 17:48:40,264 [INFO]   -> Val Loss did not improve. Patience: 6/7\n",
      "2025-05-13 17:50:03,827 [INFO] Fold 1 Phase 2 - Epoch 17/20 (Total: 24), LR: [1.0000000000000002e-07, 1e-05], Train Loss: 0.2832, Val Loss: 0.3087, Time: 83.56s\n",
      "2025-05-13 17:50:03,829 [INFO]   -> Val Loss did not improve. Patience: 7/7\n",
      "2025-05-13 17:50:03,831 [INFO]   -> Early stopping triggered at epoch 24.\n",
      "2025-05-13 17:50:03,833 [INFO] Loading best model from models_checkpointed\\best_model_fold_1.pth (Epoch 17, Val Loss: 0.2972)\n",
      "2025-05-13 17:50:04,034 [INFO] --- Evaluating Best Model for Fold 1 ---\n",
      "2025-05-13 17:50:34,505 [INFO] Fold 1 Final Validation Results (Best Model):\n",
      "2025-05-13 17:50:34,507 [INFO]   Accuracy:  0.8770\n",
      "2025-05-13 17:50:34,509 [INFO]   F1 Score:  0.7648\n",
      "2025-05-13 17:50:34,510 [INFO]   Precision: 0.7365\n",
      "2025-05-13 17:50:34,512 [INFO]   Recall:    0.7955\n",
      "2025-05-13 17:50:34,513 [INFO]   AUC:       0.9315\n",
      "2025-05-13 17:50:34,514 [INFO]   Loss:      0.2972\n",
      "2025-05-13 17:50:34,516 [INFO]   Confusion Matrix:\n",
      "2025-05-13 17:50:34,519 [INFO] \n",
      "[[9720 1027]\n",
      " [ 738 2870]]\n",
      "2025-05-13 17:50:34,520 [INFO] --- Fold 1 completed in 1732.27s ---\n",
      "2025-05-13 17:50:34,667 [INFO] --- Starting Fold 2/2 ---\n",
      "2025-05-13 17:50:34,671 [INFO] Initializing mnetv3_s model.\n",
      "c:\\Users\\Jerem\\Documents\\DHBW\\NoSmiles\\model-service\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jerem\\Documents\\DHBW\\NoSmiles\\model-service\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2025-05-13 17:50:34,749 [INFO] Adjusted Dropout rate in classifier to 0.5\n",
      "2025-05-13 17:50:34,751 [INFO] Replaced final classifier layer for 2 output classes.\n",
      "2025-05-13 17:50:34,771 [INFO] --- Phase 1: Training Head ---\n",
      "2025-05-13 17:51:12,402 [INFO] Fold 2 Phase 1 - Epoch 1/7, Train Loss: 0.6039, Time: 37.63s\n",
      "2025-05-13 17:51:50,699 [INFO] Fold 2 Phase 1 - Epoch 2/7, Train Loss: 0.5540, Time: 38.29s\n",
      "2025-05-13 17:52:31,317 [INFO] Fold 2 Phase 1 - Epoch 3/7, Train Loss: 0.5279, Time: 40.62s\n",
      "2025-05-13 17:53:08,514 [INFO] Fold 2 Phase 1 - Epoch 4/7, Train Loss: 0.5202, Time: 37.19s\n",
      "2025-05-13 17:53:46,016 [INFO] Fold 2 Phase 1 - Epoch 5/7, Train Loss: 0.5094, Time: 37.50s\n",
      "2025-05-13 17:54:23,364 [INFO] Fold 2 Phase 1 - Epoch 6/7, Train Loss: 0.5012, Time: 37.35s\n",
      "2025-05-13 17:55:00,949 [INFO] Fold 2 Phase 1 - Epoch 7/7, Train Loss: 0.4976, Time: 37.58s\n",
      "2025-05-13 17:55:00,951 [INFO] --- Phase 2: Fine-tuning Full Model ---\n",
      "2025-05-13 17:56:27,391 [INFO] Fold 2 Phase 2 - Epoch 1/20 (Total: 8), LR: [1e-05, 0.001], Train Loss: 0.4738, Val Loss: 0.3592, Time: 86.43s\n",
      "2025-05-13 17:56:27,497 [INFO]   -> New best Val Loss: 0.3592 at epoch 8. Checkpoint saved to models_checkpointed\\best_model_fold_2.pth\n",
      "2025-05-13 17:57:53,040 [INFO] Fold 2 Phase 2 - Epoch 2/20 (Total: 9), LR: [1e-05, 0.001], Train Loss: 0.4261, Val Loss: 0.3677, Time: 85.54s\n",
      "2025-05-13 17:57:53,042 [INFO]   -> Val Loss did not improve. Patience: 1/7\n",
      "2025-05-13 17:59:17,367 [INFO] Fold 2 Phase 2 - Epoch 3/20 (Total: 10), LR: [1e-05, 0.001], Train Loss: 0.3883, Val Loss: 0.3535, Time: 84.32s\n",
      "2025-05-13 17:59:17,499 [INFO]   -> New best Val Loss: 0.3535 at epoch 10. Checkpoint saved to models_checkpointed\\best_model_fold_2.pth\n",
      "2025-05-13 18:00:42,286 [INFO] Fold 2 Phase 2 - Epoch 4/20 (Total: 11), LR: [1e-05, 0.001], Train Loss: 0.3670, Val Loss: 0.3240, Time: 84.78s\n",
      "2025-05-13 18:00:42,397 [INFO]   -> New best Val Loss: 0.3240 at epoch 11. Checkpoint saved to models_checkpointed\\best_model_fold_2.pth\n",
      "2025-05-13 18:02:06,547 [INFO] Fold 2 Phase 2 - Epoch 5/20 (Total: 12), LR: [1e-05, 0.001], Train Loss: 0.3506, Val Loss: 0.2795, Time: 84.15s\n",
      "2025-05-13 18:02:06,663 [INFO]   -> New best Val Loss: 0.2795 at epoch 12. Checkpoint saved to models_checkpointed\\best_model_fold_2.pth\n",
      "2025-05-13 18:03:31,044 [INFO] Fold 2 Phase 2 - Epoch 6/20 (Total: 13), LR: [1e-05, 0.001], Train Loss: 0.3362, Val Loss: 0.3096, Time: 84.38s\n",
      "2025-05-13 18:03:31,047 [INFO]   -> Val Loss did not improve. Patience: 1/7\n",
      "2025-05-13 18:04:55,318 [INFO] Fold 2 Phase 2 - Epoch 7/20 (Total: 14), LR: [1e-05, 0.001], Train Loss: 0.3340, Val Loss: 0.4571, Time: 84.27s\n",
      "2025-05-13 18:04:55,321 [INFO]   -> Val Loss did not improve. Patience: 2/7\n",
      "2025-05-13 18:06:21,605 [INFO] Fold 2 Phase 2 - Epoch 8/20 (Total: 15), LR: [1e-05, 0.001], Train Loss: 0.3100, Val Loss: 0.2636, Time: 86.28s\n",
      "2025-05-13 18:06:21,715 [INFO]   -> New best Val Loss: 0.2636 at epoch 15. Checkpoint saved to models_checkpointed\\best_model_fold_2.pth\n",
      "2025-05-13 18:07:46,020 [INFO] Fold 2 Phase 2 - Epoch 9/20 (Total: 16), LR: [1e-05, 0.001], Train Loss: 0.2984, Val Loss: 0.2817, Time: 84.30s\n",
      "2025-05-13 18:07:46,022 [INFO]   -> Val Loss did not improve. Patience: 1/7\n",
      "2025-05-13 18:09:11,319 [INFO] Fold 2 Phase 2 - Epoch 10/20 (Total: 17), LR: [1e-05, 0.001], Train Loss: 0.2895, Val Loss: 0.3140, Time: 85.30s\n",
      "2025-05-13 18:09:11,322 [INFO]   -> Val Loss did not improve. Patience: 2/7\n",
      "2025-05-13 18:10:36,449 [INFO] Fold 2 Phase 2 - Epoch 11/20 (Total: 18), LR: [1e-05, 0.001], Train Loss: 0.2786, Val Loss: 0.2798, Time: 85.12s\n",
      "2025-05-13 18:10:36,452 [INFO]   -> Val Loss did not improve. Patience: 3/7\n",
      "2025-05-13 18:12:01,313 [INFO] Fold 2 Phase 2 - Epoch 12/20 (Total: 19), LR: [1e-05, 0.001], Train Loss: 0.2847, Val Loss: 0.2777, Time: 84.86s\n",
      "2025-05-13 18:12:01,315 [INFO]   -> Val Loss did not improve. Patience: 4/7\n",
      "2025-05-13 18:13:30,110 [INFO] Fold 2 Phase 2 - Epoch 13/20 (Total: 20), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2495, Val Loss: 0.2786, Time: 88.79s\n",
      "2025-05-13 18:13:30,113 [INFO]   -> Val Loss did not improve. Patience: 5/7\n",
      "2025-05-13 18:15:02,340 [INFO] Fold 2 Phase 2 - Epoch 14/20 (Total: 21), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2501, Val Loss: 0.2835, Time: 92.22s\n",
      "2025-05-13 18:15:02,343 [INFO]   -> Val Loss did not improve. Patience: 6/7\n",
      "2025-05-13 18:16:34,473 [INFO] Fold 2 Phase 2 - Epoch 15/20 (Total: 22), LR: [1.0000000000000002e-06, 0.0001], Train Loss: 0.2365, Val Loss: 0.2720, Time: 92.13s\n",
      "2025-05-13 18:16:34,476 [INFO]   -> Val Loss did not improve. Patience: 7/7\n",
      "2025-05-13 18:16:34,479 [INFO]   -> Early stopping triggered at epoch 22.\n",
      "2025-05-13 18:16:34,482 [INFO] Loading best model from models_checkpointed\\best_model_fold_2.pth (Epoch 15, Val Loss: 0.2636)\n",
      "2025-05-13 18:16:34,692 [INFO] --- Evaluating Best Model for Fold 2 ---\n",
      "2025-05-13 18:17:08,142 [INFO] Fold 2 Final Validation Results (Best Model):\n",
      "2025-05-13 18:17:08,144 [INFO]   Accuracy:  0.8930\n",
      "2025-05-13 18:17:08,147 [INFO]   F1 Score:  0.7861\n",
      "2025-05-13 18:17:08,149 [INFO]   Precision: 0.7898\n",
      "2025-05-13 18:17:08,152 [INFO]   Recall:    0.7824\n",
      "2025-05-13 18:17:08,155 [INFO]   AUC:       0.9392\n",
      "2025-05-13 18:17:08,158 [INFO]   Loss:      0.2636\n",
      "2025-05-13 18:17:08,161 [INFO]   Confusion Matrix:\n",
      "2025-05-13 18:17:08,164 [INFO] \n",
      "[[9996  751]\n",
      " [ 785 2822]]\n",
      "2025-05-13 18:17:08,166 [INFO] --- Fold 2 completed in 1593.50s ---\n"
     ]
    }
   ],
   "source": [
    "# --- Training & Evaluation Functions ---\n",
    "def train_one_epoch(\n",
    "    model, loader, criterion, optimizer, device, phase=\"Head\" # scaler removed\n",
    "):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch ({phase})\", leave=False)\n",
    "    for i, (imgs, labels) in enumerate(pbar):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        try:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during forward/loss calculation in training batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during backward/step in training batch {i}: {e}\")\n",
    "             optimizer.zero_grad(set_to_none=True)\n",
    "             continue\n",
    "\n",
    "        if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        else:\n",
    "            logging.warning(f\"NaN or Inf loss detected in training batch {i}. Skipping accumulation.\")\n",
    "        pbar.set_postfix(loss=loss.item() if not torch.isnan(loss) else float('nan'))\n",
    "\n",
    "    if not loader.dataset or len(loader.dataset) == 0: return 0.0\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device): # scaler removed\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    all_probs = []\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, labels) in enumerate(pbar):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            try:\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    running_loss += loss.item() * imgs.size(0)\n",
    "                else:\n",
    "                    logging.warning(f\"NaN or Inf loss detected in evaluation batch {i}. Skipping accumulation.\")\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probabilities.cpu().numpy())\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during evaluation batch {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not loader.dataset or not all_targets:\n",
    "        logging.warning(\"Evaluation dataset empty or all batches failed.\")\n",
    "        return {\n",
    "            \"loss\": float('inf'), \"accuracy\": 0.0, \"f1\": 0.0, \"precision\": 0.0,\n",
    "            \"recall\": 0.0, \"auc\": 0.0, \"cm\": np.zeros((2, 2)), \"report\": {},\n",
    "            \"preds\": np.array([]), \"targets\": np.array([])\n",
    "        }\n",
    "\n",
    "    val_loss = running_loss / len(all_targets) if len(all_targets) > 0 else float('inf')\n",
    "    all_targets_np = np.array(all_targets)\n",
    "    all_preds_np = np.array(all_preds)\n",
    "    all_probs_np = np.array(all_probs)\n",
    "\n",
    "    val_acc = accuracy_score(all_targets_np, all_preds_np) if len(all_targets_np) > 0 else 0.0\n",
    "    val_f1 = f1_score(all_targets_np, all_preds_np, average=\"binary\", zero_division=0) if len(all_targets_np) > 0 else 0.0\n",
    "    val_prec = precision_score(all_targets_np, all_preds_np, average=\"binary\", zero_division=0) if len(all_targets_np) > 0 else 0.0\n",
    "    val_rec = recall_score(all_targets_np, all_preds_np, average=\"binary\", zero_division=0) if len(all_targets_np) > 0 else 0.0\n",
    "    val_auc = 0.0\n",
    "    try:\n",
    "        if len(np.unique(all_targets_np)) > 1: # Check for at least two classes for AUC\n",
    "             val_auc = roc_auc_score(all_targets_np, all_probs_np)\n",
    "        elif len(all_targets_np) > 0: # Only one class present\n",
    "             logging.warning(\"AUC calculation skipped: only one class present in evaluation targets.\")\n",
    "    except ValueError as e: # Catch specific sklearn error if all predictions are one class\n",
    "        logging.warning(f\"AUC calculation failed (ValueError): {e}\")\n",
    "    except Exception as e: # Catch any other unexpected error\n",
    "        logging.error(f\"AUC calculation failed (General Error): {e}\")\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(all_targets_np, all_preds_np) if len(all_targets_np) > 0 else np.zeros((2,2))\n",
    "    report_dict = {}\n",
    "    if len(all_targets_np) > 0:\n",
    "        try:\n",
    "            report_dict = classification_report(\n",
    "                    all_targets_np, all_preds_np, target_names=[\"Not Happy\", \"Happy\"], output_dict=True, zero_division=0\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating classification report: {e}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": val_loss, \"accuracy\": val_acc, \"f1\": val_f1, \"precision\": val_prec,\n",
    "        \"recall\": val_rec, \"auc\": val_auc, \"cm\": cm, \"report\": report_dict,\n",
    "        \"preds\": all_preds_np, \"targets\": all_targets_np\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def evaluate_loss_only(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                running_loss += loss.item() * imgs.size(0)\n",
    "                total_samples += imgs.size(0)\n",
    "    if total_samples == 0:\n",
    "        return float('inf')\n",
    "    return running_loss / total_samples\n",
    "\n",
    "\n",
    "# --- Main K-Fold Cross-Validation ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "num_workers = 4 if os.name != \"nt\" and torch.cuda.is_available() else 0\n",
    "logging.info(f\"Using {num_workers} workers for DataLoaders\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "X_data = df_train_val.index.values\n",
    "y_data = df_train_val[\"label\"].values\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "    logging.info(f\"--- Starting Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    fold_start_time = time.time()\n",
    "\n",
    "    train_df = df_train_val.iloc[train_idx]\n",
    "    val_df = df_train_val.iloc[val_idx]\n",
    "\n",
    "    train_ds = FERDataset(train_df, transform=train_transform)\n",
    "    val_ds = FERDataset(val_df, transform=val_test_transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False, drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=False\n",
    "    )\n",
    "\n",
    "    model = get_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n",
    "\n",
    "    # --- Phase 1: Train the Head ---\n",
    "    logging.info(\"--- Phase 1: Training Head ---\")\n",
    "    if hasattr(model, 'features'):\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    if hasattr(model, 'classifier'):\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=LEARNING_RATE_HEAD,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS_PHASE1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, phase=\"Head\"\n",
    "        )\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        logging.info(\n",
    "            f\"Fold {fold+1} Phase 1 - Epoch {epoch+1}/{EPOCHS_PHASE1}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # --- Phase 2: Fine-tune the whole model ---\n",
    "    logging.info(\"--- Phase 2: Fine-tuning Full Model ---\")\n",
    "    if hasattr(model, 'features'):\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\n",
    "            \"params\": model.features.parameters() if hasattr(model, 'features') else [],\n",
    "            \"lr\": LEARNING_RATE_BACKBONE,\n",
    "        },\n",
    "        {\n",
    "            \"params\": model.classifier.parameters() if hasattr(model, 'classifier') else model.parameters(),\n",
    "            \"lr\": LEARNING_RATE_HEAD,\n",
    "        },\n",
    "    ],\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_epoch = -1\n",
    "    checkpoint_path = os.path.join(MODEL_DIR, f\"best_model_fold_{fold+1}.pth\")\n",
    "\n",
    "    for epoch in range(EPOCHS_PHASE2):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_total = EPOCHS_PHASE1 + epoch + 1\n",
    "\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, phase=\"Full\"\n",
    "        )\n",
    "        val_loss = evaluate_loss_only(model, val_loader, criterion, device)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        logging.info(\n",
    "            f\"Fold {fold+1} Phase 2 - Epoch {epoch+1}/{EPOCHS_PHASE2} (Total: {current_epoch_total}), \"\n",
    "            f\"LR: {current_lrs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "            f\"Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_epoch = current_epoch_total\n",
    "            try:\n",
    "                torch.save({\n",
    "                    'epoch': current_epoch_total,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                }, checkpoint_path)\n",
    "                logging.info(\n",
    "                    f\"  -> New best Val Loss: {best_val_loss:.4f} at epoch {best_epoch}. Checkpoint saved to {checkpoint_path}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving checkpoint: {e}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            logging.info(f\"  -> Val Loss did not improve. Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            logging.info(f\"  -> Early stopping triggered at epoch {current_epoch_total}.\")\n",
    "            break\n",
    "\n",
    "    # Load best model and do full evaluation (all metrics) at the end of the fold\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        logging.info(f\"Loading best model from {checkpoint_path} (Epoch {best_epoch}, Val Loss: {best_val_loss:.4f})\")\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading checkpoint: {e}. Using last model state.\")\n",
    "    else:\n",
    "        logging.warning(\"No best model checkpoint found for this fold. Using last model state.\")\n",
    "\n",
    "    logging.info(f\"--- Evaluating Best Model for Fold {fold+1} ---\")\n",
    "    final_fold_metrics = evaluate(model, val_loader, criterion, device)  # Full metrics only here\n",
    "\n",
    "    logging.info(f\"Fold {fold+1} Final Validation Results (Best Model):\")\n",
    "    logging.info(f\"  Accuracy:  {final_fold_metrics['accuracy']:.4f}\")\n",
    "    logging.info(f\"  F1 Score:  {final_fold_metrics['f1']:.4f}\")\n",
    "    logging.info(f\"  Precision: {final_fold_metrics['precision']:.4f}\")\n",
    "    logging.info(f\"  Recall:    {final_fold_metrics['recall']:.4f}\")\n",
    "    logging.info(f\"  AUC:       {final_fold_metrics['auc']:.4f}\")\n",
    "    logging.info(f\"  Loss:      {final_fold_metrics['loss']:.4f}\")\n",
    "    logging.info(\"  Confusion Matrix:\")\n",
    "    logging.info(f\"\\n{final_fold_metrics['cm']}\")\n",
    "\n",
    "    fold_results.append(final_fold_metrics)\n",
    "    fold_time = time.time() - fold_start_time\n",
    "    logging.info(f\"--- Fold {fold+1} completed in {fold_time:.2f}s ---\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:49:35,342 [INFO] --- Cross-Validation Summary ---\n",
      "2025-05-13 18:49:35,343 [INFO] Mean Accuracy:  0.8850 ± 0.0080\n",
      "2025-05-13 18:49:35,345 [INFO] Mean F1 Score:  0.7754 ± 0.0106\n",
      "2025-05-13 18:49:35,345 [INFO] Mean Precision: 0.7631 ± 0.0267\n",
      "2025-05-13 18:49:35,346 [INFO] Mean Recall:    0.7889 ± 0.0065\n",
      "2025-05-13 18:49:35,347 [INFO] Mean AUC:       0.9354 ± 0.0039\n",
      "2025-05-13 18:49:35,348 [INFO] --- Training Final Model on Full Train/Val Dataset ---\n",
      "2025-05-13 18:49:35,348 [INFO] Initializing mnetv3_s model.\n",
      "c:\\Users\\Jerem\\Documents\\DHBW\\NoSmiles\\model-service\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jerem\\Documents\\DHBW\\NoSmiles\\model-service\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2025-05-13 18:49:35,563 [INFO] Adjusted Dropout rate in classifier to 0.5\n",
      "2025-05-13 18:49:35,565 [INFO] Replaced final classifier layer for 2 output classes.\n",
      "2025-05-13 18:49:35,591 [INFO] --- Final Model - Phase 1: Training Head ---\n",
      "2025-05-13 18:50:45,175 [INFO] Final Phase 1 - Epoch 1/7, Train Loss: 0.5753, Time: 69.58s\n",
      "2025-05-13 18:51:58,053 [INFO] Final Phase 1 - Epoch 2/7, Train Loss: 0.5230, Time: 72.88s\n",
      "2025-05-13 18:53:06,128 [INFO] Final Phase 1 - Epoch 3/7, Train Loss: 0.5085, Time: 68.08s\n",
      "2025-05-13 18:54:19,115 [INFO] Final Phase 1 - Epoch 4/7, Train Loss: 0.4997, Time: 72.99s\n",
      "2025-05-13 18:55:34,379 [INFO] Final Phase 1 - Epoch 5/7, Train Loss: 0.4911, Time: 75.26s\n",
      "2025-05-13 18:56:49,978 [INFO] Final Phase 1 - Epoch 6/7, Train Loss: 0.4846, Time: 75.60s\n",
      "2025-05-13 18:58:04,280 [INFO] Final Phase 1 - Epoch 7/7, Train Loss: 0.4787, Time: 74.30s\n",
      "2025-05-13 18:58:04,281 [INFO] --- Final Model - Phase 2: Fine-tuning Full Model ---\n",
      "2025-05-13 18:59:55,069 [INFO] Final Phase 2 - Epoch 1/20, Train Loss: 0.4400, Time: 110.78s\n",
      "2025-05-13 19:01:44,920 [INFO] Final Phase 2 - Epoch 2/20, Train Loss: 0.3798, Time: 109.85s\n",
      "2025-05-13 19:03:32,003 [INFO] Final Phase 2 - Epoch 3/20, Train Loss: 0.3513, Time: 107.08s\n",
      "2025-05-13 19:05:21,529 [INFO] Final Phase 2 - Epoch 4/20, Train Loss: 0.3293, Time: 109.52s\n",
      "2025-05-13 19:07:10,595 [INFO] Final Phase 2 - Epoch 5/20, Train Loss: 0.3111, Time: 109.06s\n",
      "2025-05-13 19:08:56,889 [INFO] Final Phase 2 - Epoch 6/20, Train Loss: 0.2994, Time: 106.29s\n",
      "2025-05-13 19:10:43,477 [INFO] Final Phase 2 - Epoch 7/20, Train Loss: 0.2894, Time: 106.59s\n",
      "2025-05-13 19:12:30,118 [INFO] Final Phase 2 - Epoch 8/20, Train Loss: 0.2773, Time: 106.64s\n",
      "2025-05-13 19:14:16,740 [INFO] Final Phase 2 - Epoch 9/20, Train Loss: 0.2694, Time: 106.62s\n",
      "2025-05-13 19:16:04,967 [INFO] Final Phase 2 - Epoch 10/20, Train Loss: 0.2637, Time: 108.22s\n",
      "2025-05-13 19:17:54,128 [INFO] Final Phase 2 - Epoch 11/20, Train Loss: 0.2536, Time: 109.16s\n",
      "2025-05-13 19:19:44,327 [INFO] Final Phase 2 - Epoch 12/20, Train Loss: 0.2479, Time: 110.20s\n",
      "2025-05-13 19:21:32,644 [INFO] Final Phase 2 - Epoch 13/20, Train Loss: 0.2392, Time: 108.31s\n",
      "2025-05-13 19:23:21,185 [INFO] Final Phase 2 - Epoch 14/20, Train Loss: 0.2378, Time: 108.54s\n",
      "2025-05-13 19:25:09,117 [INFO] Final Phase 2 - Epoch 15/20, Train Loss: 0.2291, Time: 107.93s\n",
      "2025-05-13 19:26:57,890 [INFO] Final Phase 2 - Epoch 16/20, Train Loss: 0.2222, Time: 108.77s\n",
      "2025-05-13 19:28:49,429 [INFO] Final Phase 2 - Epoch 17/20, Train Loss: 0.2212, Time: 111.54s\n",
      "2025-05-13 19:30:38,813 [INFO] Final Phase 2 - Epoch 18/20, Train Loss: 0.2184, Time: 109.38s\n",
      "2025-05-13 19:32:28,223 [INFO] Final Phase 2 - Epoch 19/20, Train Loss: 0.2069, Time: 109.41s\n",
      "2025-05-13 19:34:18,889 [INFO] Final Phase 2 - Epoch 20/20, Train Loss: 0.2068, Time: 110.66s\n",
      "2025-05-13 19:34:18,948 [INFO] Final model state_dict saved to models_checkpointed\\mnetv3_s_fer2013_happy_final.pth\n"
     ]
    }
   ],
   "source": [
    "# --- Cross-Validation Summary ---\n",
    "logging.info(\"--- Cross-Validation Summary ---\")\n",
    "if fold_results:\n",
    "    accs = [r[\"accuracy\"] for r in fold_results]\n",
    "    f1s = [r[\"f1\"] for r in fold_results]\n",
    "    precs = [r[\"precision\"] for r in fold_results]\n",
    "    recs = [r[\"recall\"] for r in fold_results]\n",
    "    aucs = [r[\"auc\"] for r in fold_results]\n",
    "\n",
    "    logging.info(f\"Mean Accuracy:  {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    logging.info(f\"Mean F1 Score:  {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n",
    "    logging.info(f\"Mean Precision: {np.mean(precs):.4f} ± {np.std(precs):.4f}\")\n",
    "    logging.info(f\"Mean Recall:    {np.mean(recs):.4f} ± {np.std(recs):.4f}\")\n",
    "    logging.info(f\"Mean AUC:       {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "else:\n",
    "    logging.warning(\"No fold results to summarize.\")\n",
    "\n",
    "# --- Training Final Model on Full Train/Val Data ---\n",
    "logging.info(\"--- Training Final Model on Full Train/Val Dataset ---\")\n",
    "final_model = get_model().to(device)\n",
    "final_ds = FERDataset(df_train_val, transform=train_transform)\n",
    "final_loader = DataLoader(\n",
    "    final_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False, drop_last=True\n",
    ")\n",
    "criterion_final = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n",
    "# scaler_final is removed\n",
    "\n",
    "# --- Final Model - Phase 1: Train the Head ---\n",
    "logging.info(\"--- Final Model - Phase 1: Training Head ---\")\n",
    "if hasattr(final_model, 'features'):\n",
    "    for param in final_model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "if hasattr(final_model, 'classifier'):\n",
    "    for param in final_model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer_head_final = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, final_model.parameters()),\n",
    "    lr=LEARNING_RATE_HEAD,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "for epoch in range(EPOCHS_PHASE1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_one_epoch(\n",
    "        final_model, final_loader, criterion_final, optimizer_head_final, device, phase=\"Head\" # scaler removed\n",
    "    )\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    logging.info(\n",
    "        f\"Final Phase 1 - Epoch {epoch+1}/{EPOCHS_PHASE1}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s\"\n",
    "    )\n",
    "\n",
    "# --- Final Model - Phase 2: Fine-tune the whole model ---\n",
    "logging.info(\"--- Final Model - Phase 2: Fine-tuning Full Model ---\")\n",
    "if hasattr(final_model, 'features'):\n",
    "    for param in final_model.features.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\n",
    "            \"params\": final_model.features.parameters() if hasattr(final_model, 'features') else [],\n",
    "            \"lr\": LEARNING_RATE_BACKBONE,\n",
    "        },\n",
    "        {\n",
    "            \"params\": final_model.classifier.parameters() if hasattr(final_model, 'classifier') else final_model.parameters(),\n",
    "            \"lr\": LEARNING_RATE_HEAD,\n",
    "        },\n",
    "    ],\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Optionally, you can use a validation split from df_train_val for early stopping,\n",
    "# or just train for all epochs if you want to use all data.\n",
    "\n",
    "for epoch in range(EPOCHS_PHASE2):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_one_epoch(\n",
    "        final_model, final_loader, criterion_final, optimizer, device, phase=\"Full\"\n",
    "    )\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    logging.info(\n",
    "        f\"Final Phase 2 - Epoch {epoch+1}/{EPOCHS_PHASE2}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s\"\n",
    "    )\n",
    "\n",
    "final_model_save_path = os.path.join(MODEL_DIR, f\"{BASE_MODEL_NAME.lower()}_fer2013_happy_final.pth\")\n",
    "try:\n",
    "    torch.save(final_model.state_dict(), final_model_save_path)\n",
    "    logging.info(f\"Final model state_dict saved to {final_model_save_path}\")\n",
    "    del final_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving final model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 19:34:32,967 [INFO] --- Evaluating Final Model on PrivateTest Set ---\n",
      "2025-05-13 19:34:32,969 [INFO] Initializing mnetv3_s model.\n",
      "2025-05-13 19:34:33,047 [INFO] Adjusted Dropout rate in classifier to 0.5\n",
      "2025-05-13 19:34:33,049 [INFO] Replaced final classifier layer for 2 output classes.\n",
      "2025-05-13 19:34:33,218 [INFO] Successfully loaded final model from models_checkpointed\\mnetv3_s_fer2013_happy_final.pth\n",
      "2025-05-13 19:34:33,219 [INFO] --- Calculating Performance Metrics on Test Set ---\n",
      "2025-05-13 19:34:53,412 [INFO] Final Test Set Performance Metrics:\n",
      "2025-05-13 19:34:53,413 [INFO]   Accuracy:  0.9153\n",
      "2025-05-13 19:34:53,414 [INFO]   F1 Score:  0.8382\n",
      "2025-05-13 19:34:53,415 [INFO]   Precision: 0.7939\n",
      "2025-05-13 19:34:53,416 [INFO]   Recall:    0.8878\n",
      "2025-05-13 19:34:53,416 [INFO]   AUC:       0.9645\n",
      "2025-05-13 19:34:53,416 [INFO]   Loss:      0.2214\n",
      "2025-05-13 19:34:53,418 [INFO]   Confusion Matrix:\n",
      "2025-05-13 19:34:53,418 [INFO] \n",
      "[[4995  409]\n",
      " [ 199 1575]]\n",
      "2025-05-13 19:34:53,419 [INFO]   Classification Report:\n",
      "2025-05-13 19:34:53,428 [INFO] \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not Happy       0.96      0.92      0.94      5404\n",
      "       Happy       0.79      0.89      0.84      1774\n",
      "\n",
      "    accuracy                           0.92      7178\n",
      "   macro avg       0.88      0.91      0.89      7178\n",
      "weighted avg       0.92      0.92      0.92      7178\n",
      "\n",
      "2025-05-13 19:34:53,429 [INFO] --- Performing Inference Speed Test on Test Set ---\n",
      "2025-05-13 19:34:53,484 [INFO] Performing warm-up inferences...\n",
      "2025-05-13 19:34:53,557 [INFO] Warm-up complete.\n",
      "2025-05-13 19:34:53,558 [INFO] Starting timed inferences...\n",
      "Inference Speed Test: 100%|██████████| 225/225 [00:14<00:00, 15.96it/s]\n",
      "2025-05-13 19:35:07,658 [INFO] Inference Speed Test Results:\n",
      "2025-05-13 19:35:07,659 [INFO]   Total images processed: 7178\n",
      "2025-05-13 19:35:07,660 [INFO]   Total inference time: 3.2083 seconds\n",
      "2025-05-13 19:35:07,660 [INFO]   Average inference time per batch: 0.014259 seconds (Batch Size: 32)\n",
      "2025-05-13 19:35:07,662 [INFO]   Average inference time per image: 0.000447 seconds\n",
      "2025-05-13 19:35:07,662 [INFO]   Images Per Second (IPS): 2237.33\n"
     ]
    }
   ],
   "source": [
    "# --- Final Evaluation on Test Set ---\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "final_model_save_path=os.path.join(MODEL_DIR, f\"{BASE_MODEL_NAME.lower()}_fer2013_happy_final.pth\")\n",
    "logging.info(\"--- Evaluating Final Model on PrivateTest Set ---\")\n",
    "if len(df_test) > 0:\n",
    "    test_ds = FERDataset(df_test, transform=val_test_transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        eval_model = get_model().to(device) # Create a new model instance for evaluation\n",
    "        eval_model.load_state_dict(torch.load(final_model_save_path, map_location=device))\n",
    "        eval_model.eval() # Ensure model is in eval mode\n",
    "        logging.info(f\"Successfully loaded final model from {final_model_save_path}\")\n",
    "\n",
    "        # 1. Standard Metrics Evaluation\n",
    "        logging.info(\"--- Calculating Performance Metrics on Test Set ---\")\n",
    "        test_metrics = evaluate(eval_model, test_loader, criterion_final, device)\n",
    "\n",
    "        logging.info(\"Final Test Set Performance Metrics:\")\n",
    "        logging.info(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "        logging.info(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "        logging.info(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "        logging.info(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "        logging.info(f\"  AUC:       {test_metrics['auc']:.4f}\")\n",
    "        logging.info(f\"  Loss:      {test_metrics['loss']:.4f}\")\n",
    "        logging.info(\"  Confusion Matrix:\")\n",
    "        logging.info(f\"\\n{test_metrics['cm']}\")\n",
    "        logging.info(\"  Classification Report:\")\n",
    "        if test_metrics['targets'].size > 0:\n",
    "             logging.info(f\"\\n{classification_report(test_metrics['targets'], test_metrics['preds'], target_names=['Not Happy', 'Happy'], zero_division=0)}\")\n",
    "        else:\n",
    "             logging.warning(\"No predictions generated during final test evaluation for report.\")\n",
    "\n",
    "        # 2. Inference Speed Test\n",
    "        logging.info(\"--- Performing Inference Speed Test on Test Set ---\")\n",
    "        dummy_batch = None\n",
    "        try:\n",
    "            dummy_batch = next(iter(test_loader)) # Get one batch for warm-up\n",
    "        except StopIteration:\n",
    "            logging.warning(\"Test loader is empty, cannot perform warm-up for speed test.\")\n",
    "        \n",
    "        if dummy_batch:\n",
    "            dummy_imgs, _ = dummy_batch\n",
    "            dummy_imgs = dummy_imgs.to(device)\n",
    "\n",
    "            # Warm-up iterations\n",
    "            logging.info(\"Performing warm-up inferences...\")\n",
    "            with torch.no_grad():\n",
    "                for _ in range(5): # Number of warm-up iterations\n",
    "                    _ = eval_model(dummy_imgs)\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "            logging.info(\"Warm-up complete.\")\n",
    "\n",
    "        total_inference_time = 0\n",
    "        total_images_processed = 0\n",
    "        inference_times_per_batch = []\n",
    "\n",
    "        logging.info(\"Starting timed inferences...\")\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in tqdm(test_loader, desc=\"Inference Speed Test\"):\n",
    "                imgs = imgs.to(device)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                start_time = time.perf_counter() # More precise timer\n",
    "\n",
    "                _ = eval_model(imgs)\n",
    "\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                end_time = time.perf_counter()\n",
    "\n",
    "                batch_time = end_time - start_time\n",
    "                inference_times_per_batch.append(batch_time)\n",
    "                total_inference_time += batch_time\n",
    "                total_images_processed += imgs.size(0)\n",
    "        \n",
    "        if total_images_processed > 0:\n",
    "            avg_time_per_image = total_inference_time / total_images_processed\n",
    "            images_per_second = total_images_processed / total_inference_time\n",
    "            avg_time_per_batch = total_inference_time / len(test_loader)\n",
    "\n",
    "            logging.info(\"Inference Speed Test Results:\")\n",
    "            logging.info(f\"  Total images processed: {total_images_processed}\")\n",
    "            logging.info(f\"  Total inference time: {total_inference_time:.4f} seconds\")\n",
    "            logging.info(f\"  Average inference time per batch: {avg_time_per_batch:.6f} seconds (Batch Size: {BATCH_SIZE})\")\n",
    "            logging.info(f\"  Average inference time per image: {avg_time_per_image:.6f} seconds\")\n",
    "            logging.info(f\"  Images Per Second (IPS): {images_per_second:.2f}\")\n",
    "        else:\n",
    "            logging.warning(\"No images were processed during the inference speed test (Test set might be empty or all batches failed).\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Final model file not found at {final_model_save_path}. Cannot evaluate on test set.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during final evaluation or speed test on test set: {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logging.warning(\"PrivateTest set is empty or could not be loaded. Skipping final evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
